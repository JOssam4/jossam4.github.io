<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>ENGL 109 Essay 3</title>
    <link rel="stylesheet" href="essay3Styles.css">
</head>
<body>
    <div id="root">
        <div id="content">
            <h1 id="title">How Social Media Makes Ideas More Dangerous</h1>
            <h2 id="author">James Ossam</h2>
            <h3 id="date">6/8/22</h3>
            <p class="essay">
                On Saturday, May 14th, 2022, eighteen-year-old gunman Payton Gendron shot and killed ten people in a Buffalo, New York supermarket, along with a security guard. Before committing this atrocity, the gunman wrote a manifesto, which has since been posted online. It is one hundred eighty pages in length, and fixates on an idea known as ‘replacement theory,’ which states that white people are being ‘slowly and intentionally replaced by minorities and immigrants.’ This manifesto “is rife with pseudo-scientific racism, antisemitic conspiracy theories and a call for others to mimic his violence.” It is also “mostly plagiarized from other extremists and from the far-right 4chan” (Wolf). It is important to examine which preventable factors lead him to hold such ludicrous ideas to prevent others from going down the same path.
            </p>
            <figure id="gendron">
                <img src="./images/Gendron.png" alt="Buffalo Shooter Payton Gendron in police custody">
                <figcaption>Buffalo Shooter Payton Gendron in police custody</figcaption>
            </figure>
            <p class="essay">
                Gendron’s writings leading up to the event suggest that the attack could have been prevented if he had never found his way into such a cesspool of such violent and hateful ideas without basis in reality. Thus, I claim that digital technology in the form of social media makes it far too easy for people to find information that supports their beliefs, which in turn leads to them holding more radical and dangerous beliefs. I will support this argument by supporting the following sub-claims: that digital technology allows people to form echo chambers where contradictory ideas are filtered out, that the algorithms social media platforms employ give credibility to the wrong voices, and that the two work together to convince people that the world looks different than they would find if they talked to people in real life.
            </p>
            <p class="essay">
                First of all, social media platforms encourage people to form echo chambers online. An echo chamber refers to “an environment in which a person encounters only beliefs or opinions that coincide with their own, so that their existing views are reinforced and alternative ideas are not considered” (Oxford Dictionary). This concept goes by many names online: Facebook groups, communities on Reddit (subreddits), and to some extent hashtags on platforms such as Instagram, Twitter, or Tiktok.
            </p>
            <figure id="removed_comment">
                <img src="./images/reddit_removed_comment.png" alt="image of removed comment on Reddit">
                <figcaption>On Reddit, moderators can remove information they do not like, and do not have to provide justification for doing so.</figcaption>
            </figure>
            <p class="essay">
                According to an article that talks about the nature of these isolated groups online, “users have become ensconced within specific, self-selected groups, which means that news and views are shared nearly exclusively with like-minded users.” This is problematic because it “creates an aura of expertise and trustworthiness where those factors would not normally exist” (Commanding the Trend, 10). Thus, “this ‘echo chamber’ can promote the scenario in which your friend is ‘just as much a source of insightful analysis on the nuances of U.S. foreign policy towards Iran as regional scholars, arms control experts, or journalists covering the State Department’ (Commanding the Trend, 10). So, valuable opinions, such as those of experts, are treated with the same degree of validity as those of non-experts, or even worse, those who know nothing about the issue they are writing about. This means that meaningful opinions have to share the stage with the ramblings of people without relevant knowledge, which highlights one of the biggest reasons that ideas become so polarized in online spaces.
            </p>
            <p class="essay">
                Gendron wrote that “‘extreme boredom’ drove him to <a href="https://www.4chan.org/">4chan</a> in March 2020,” the same month pandemic lockdowns started. 4chan is a site where “even conversations on a board devoted to cooking frequently veer into racist slurs and junk race science” (Ling). According to chat logs, Gendron said he “only really turned racist when 4chan started giving [him] facts” (Ling). He also explained that “only 4chan—including the board dedicated to Nazi ideology—gave him the real news he sought” (Ling). He also “confessed to browsing 4chan daily and that he ‘barely interacts with regular people’” (Ling). Perhaps most worryingly, Gendron wrote that “every time [he thought] maybe [he] shouldn’t commit to an attack [he spent] 5 min on /pol/, then [his] motivation returns” (Ling). One might see how hateful echo chambers can cause real-world problems. Gendron repeatedly questioned if he could commit such an atrocity, but found motivation from this online group. The nature of this echo chamber also prevents people from seeing evidence that contradicts these claims, so they are more likely to believe these myths are facts. Once people start believing these myths are true, they will reject more reliable sources of media that contradict these claims and will realize that the echo chamber is their only source of ‘real’ information.
            </p>
            <p class="essay">
                This realization goes hand-in-hand with the fact that Americans are often unable to distinguish between real and fake news. <a href="https://www.buzzfeednews.com/article/craigsilverman/fake-news-survey">A survey conducted in 2016</a> found that both Republicans and Democrats tend to perceive fake news headlines as accurate. The survey found that “roughly 84% of the time, Republicans rated fake news headlines as accurate compared to a rate of 71% among Democrats” (Silverman and Singer-Vine). This finding suggests that Americans are not nearly as good at sniffing out fake news as they think they are. This helps explain why people like Gendron believed the information presented to him on 4chan so readily without realizing it was fake. Especially since the views Gendron held align much more closely with the Republican party than the Democratic party, the findings from this survey illustrate how disinformation is perceived as the truth, and how dangerous it can be.
            </p>
            <p class="essay">
                Another reason radical ideas are so widespread on the internet is due to the algorithms created by social media companies. These algorithms maximize the amount of time users spend on their sites and apps, and turn a blind eye to how this is done. For example, algorithms on Facebook’s website will show users headlines intended to cause an emotional reaction. Angry reactions are associated with longer engagement on the platform, so the Facebook algorithm has ‘learned’ to show users headlines that will evoke this reaction. According to a study done by NPR, “far-right accounts known for spreading misinformation are not only thriving on Facebook, they’re actually more successful than other kinds of accounts at getting likes, shares and other forms of user engagement” (Martin and Jarvis). These accounts get “almost twice as much engagement per follower” compared to other sources known for spreading misinformation. Since Facebook’s algorithm ‘wants’ to maximize engagement, it will show these accounts to more users, which in turn leads to these harmful ideas being spread far more than they otherwise would be. This means that harmful and radical ideas are facilitated by social media platforms instead of stopped in their tracks as they should be.
            </p>
            <figure id="angry">
                <img src="./images/angry.jpeg" alt="Facebook's angry reaction icon">
                <figcaption>Believe it or not, this cute angry icon is a major driver of disinformation on Facebook.</figcaption>
            </figure>
            <p class="essay">
                Social media companies have very little incentive to stop these ideas from spreading. Since harmful ideas generate more user engagement, platforms are incentivized to turn a blind eye to this issue. After all, these ideas increase user engagement, which in turn generates profit. Thus, platforms such as Reddit or Facebook often will not intervene until too late, when the community has established itself, has a considerable amount of members, and can migrate to another platform. Numerous problematic communities have been able to successfully migrate to other platforms. I have been a Reddit user for about seven years, and during that time I have seen many communities face disciplinary action, with some being outright banned. One of the biggest communities banned recently was the subreddit dedicated to Donald Trump called r/The_Donald. It started as a community of Trump supporters and ended up being filled with racism, misogyny, and Islamophobia. When Reddit finally cracked down and banned the community, moderators created a new website and moved the community there. Similar communities often do not disperse or go away. They simply migrate to platforms with even fewer rules regarding acceptable ideas, such as 4chan.
            </p>
            <p class="essay">
                In a journal detailing how extremism spreads so easily online, authors Alexandra Evans and Heather Williams note that “the number of people exposed to radical ideas has risen with the growth of the number of internet users and the popularization of message forums, social media networks, and other virtual communities.” They go on to say that the “percentage of the population that subscribes to radical ideologies is expected to increase–and some subset of that population will go so far as to use violence to promote their ideas” (Evans & Williams, 13). Worryingly, this article was published in early April of 2022, and this shooting took place six weeks later. It seems as if the authors’ prediction is coming true much sooner than expected.
            </p>
            <p class="essay">
                The addictiveness of social media platforms leads to people spending more time on their devices than they otherwise would, which means they are more likely to see and engage with radical ideas online. According to a study on the amount of time American youth spend consuming media, “the average American eight- to eighteen-year-old [reports] more than six hours of daily media use” (Trends in Media Use, 1). This figure was from 2008, so we can expect that it has risen in the last fourteen years. Especially considering how big social media companies have gotten since then, we can assume that American youth spend much more time consuming media than they ever have before. Since the rise of virtual meeting platforms from the COVID-19 pandemic, it is becoming increasingly common for people of all ages to interact with other people primarily through the internet. This suggests a drop in face-to-face communication.
            </p>
            <figure id="cell_phones">
                <img src="./images/youth_cell_phones.jpeg" alt="A group of teenagers on their mobile devices">
                <figcaption>More and more young Americans are consuming digital media instead of interacting face-to-face.</figcaption>
            </figure>
            <p class="essay">
                Lack of face-to-face communication makes it harder for people caught in echo chambers to see the world for anything other than what the echo chamber leads them to believe. Gendron wrote that he “barely interact[ed] with regular people” in early 2022 (Ling). He also admitted to browsing these racist platforms daily. Since people are usually influenced by those around them, one can see how such hateful ideas could rub off on unsuspecting viewers. As users spend more time engaging with this sort of media, they begin to lose the ability to differentiate between opinion and fact. Users who accept hateful myths such as ‘replacement theory’ as fact will likely hold opinions that are even farther from reality. Also, they will ironically tend to reject any evidence that contradicts this claim as ‘fake news.’
            </p>
            <p class="essay">
                Luckily, this drastic outcome is preventable. Authors Evans and Williams note that many institutions are already hoping to stop the current trend toward extremism. They state that “numerous governmental, educational, and civil sector entities seek to disrupt extremists’ attempts to exploit the internet and to impede the indoctrination of individuals online” (Evans & Williams, 13). These entities aim to do so by both removing this content as it appears and denying these extremist groups access to virtual platforms from which they can generate revenue and support their cause. They also hope to use “strategic communication campaigns to prevent radicalization, promote community resiliency, and aid the deradicalization and reintegration of extremist adherents” (Evans & Williams, 14). So, while disinformation online is extremely harmful, there are still ways to combat it.
            </p>
            <p class="essay">
                However, the authors also note that censorship efforts from government entities will be met with strong opposition from these malicious actors. After all, a major element of their narratives involves telling their members not to trust any information promoted by mainstream media, and that the government does not want them to know these ‘facts’ (myths). Thus, these groups will see any attempt to remove this content as further proof of the validity of their ideas. After all, if a content creator tells a viewer that the government does not want this information to spread, and then the government removes said media, the viewer will think the creator was correct which makes the nonsensical information seem more valid than it is.
            </p>
            <p class="essay">
                Through the facilitation of online echo chambers, by promoting ideas that cause reactions instead of those that are more true, and by transitioning society to prefer online communication, social media platforms have made spreading harmful misinformation and disinformation as easy as ever. Since everyone’s ideas are treated equally online, harmful ideas are treated as equal to harmless ones. When these concepts are combined with the fact that these algorithms show everyone different results, we can see how social media platforms give us the illusion of connection while also separating us from each other. As ideas we know to be harmful and radical become more mainstream, it will be harder to tell which ideas come from sensical reasoning and which do not. Thus, it is more important than ever for social media companies to carefully moderate the information they show users. Failure to do so could have widespread negative consequences that could lead to a breakdown in society.
            </p>
            <br>
            <br>
            <h3>Works Cited:</h3>
            <div id="works-cited">
                <p class="citation">Evans, Alexandra T., and Heather J. Williams. “Countering Virtual Extremism.” How Extremism Operates Online: A Primer, RAND Corporation, 2022, pp. 13-15, <a href="http://www.jstor.org/stable/resrep40325.7" target="_blank">http://www.jstor.org/stable/resrep40325.7</a>. Accessed 16 May 2022.</p>
                <p class="citation">Ling, Justin. "How 4Chan’S Toxic Culture Helped Radicalize Buffalo Shooting Suspect". The Guardian, 2022, <a href="https://www.theguardian.com/us-news/2022/may/18/4chan-radicalize-buffalo-shooting-white-supremacy" target="_blank">https://www.theguardian.com/us-news/2022/may/18/4chan-radicalize-buffalo-shooting-white-supremacy</a>.</p>
                <p class="citation">Martin, Michel, and Will Jarvis. "Far-Right Misinformation Is Thriving On Facebook. A New Study Shows Just How Much". Npr.Org, 2021, <a href="https://www.npr.org/2021/03/06/974394783/far-right-misinformation-is-thriving-on-facebook-a-new-study-shows-just-how-much." target="_blank">https://www.npr.org/2021/03/06/974394783/far-right-misinformation-is-thriving-on-facebook-a-new-study-shows-just-how-much</a>.</p>
                <p class="citation">Prier, Jarred. “Commanding the Trend: Social Media as Information Warfare.” Strategic Studies Quarterly, vol. 11, no. 4, 2017, pp. 50–85. JSTOR, <a href="http://www.jstor.org/stable/26271634" target="_blank">http://www.jstor.org/stable/26271634</a>. Accessed 18 May 2022.</p>
                <p class="citation">Roberts, Donald F., and Ulla G. Foehr. “Trends in Media Use.” The Future of Children, vol. 18, no. 1, 2008, pp. 11–37. JSTOR, <a href="http://www.jstor.org/stable/20053118" target="_blank">http://www.jstor.org/stable/20053118</a>. Accessed 18 May 2022.</p>
                <p class="citation">Silverman, Craig, and Jeremy Singer-Vine. "Most Americans Who See Fake News Believe It, New Survey Says". Buzzfeed News, 2016, <a href="https://www.buzzfeednews.com/article/craigsilverman/fake-news-survey" target="_blank">https://www.buzzfeednews.com/article/craigsilverman/fake-news-survey</a>.</p>
                <p class="citation">Wolf, Zachary. "'Replacement' Conspiracies Driving Gunmen Creep Into Mainstream Politics". CNN, 2022, <a href="https://www.cnn.com/2022/05/16/politics/replacement-theory-buffalo-what-matters/index.html" target="_blank">https://www.cnn.com/2022/05/16/politics/replacement-theory-buffalo-what-matters/index.html</a>.</p>
            </div>
        </div>
    </div>
</body>
</html>
